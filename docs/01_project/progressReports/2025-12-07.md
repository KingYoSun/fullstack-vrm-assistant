# 2025年12月07日 進捗レポート

## 概要
- LLM 応答のデフォルトプロンプトを会話調/150字以内に統一し、キャラクター設定とシステムプロンプトを DB で管理・適用できるようにした。
- DGX Spark 向けに gpt-oss-20b (llama.cpp/llama-server) をビルド・起動する構成へ更新し、LLM 応答ストリームの `None` 混入を解消した。

## 成果
- partial STT レイテンシの p95 が 500ms 未満であることを dev/prod compose 環境で確認し、計測ログを取得した。
- 発話停止から TTS 再生開始までの p95 が 2s 未満であることを複数サンプルで確認し、ログを残した。
- VRM 表示とリップシンクが正常に動作し、任意の VRM 切替を含めて録画で確認した。
- サンプル文書を ingest した RAG 応答で引用/文脈が含まれることを確認し、request_id と出典断片を記録した。
- `COMPOSE_PROFILES=dev|prod docker compose up` で一発起動し、`config/providers.yaml` から環境変数/エンドポイントが正しく解決されることを ready/health で確認した。
- Diagnostics RAG で例外内容を構造化ログに出力するよう改善し、原因追跡が容易になった。
- WebSocket 音声会話が STT→LLM→TTS→ブラウザ再生まで一通り通ることを dev compose 起動下で確認（レイテンシ計測と証跡取得は未実施）。
- プロンプトビルダーを追加し、WS/Text/Diagnostics すべてでキャラ設定＋RAG 文脈を含むシステムプロンプトを共通化。150 文字超はストリームを打ち切りつつクランプして短文化。
- Postgres に CharacterProfile テーブルを追加し、`/api/v1/characters` CRUD API を実装（名前重複は 409 を返却）。
- Postgres に SystemPrompt テーブルを追加し、`/api/v1/system-prompts` CRUD + active 切替を実装。取得できない場合はデフォルト文へフォールバック。
- フロントに「Conversation Persona」パネルを追加し、キャラの作成/更新/削除・適用を UI で完結。選択したキャラ ID を WS 接続と diagnostics/llm リクエストに付与し、プロンプトへ反映できるようにした。
- フロントにシステムプロンプト管理 UI を追加し、保存/適用/削除をブラウザから操作可能にした。
- LLM を gpt-oss-20b GGUF + llama-server (CUDA 13.0 / SM 12.1) に刷新し、`docker/llm-llama/Dockerfile` と compose の llm サービスを追加。`.env.default` / README / production_runtime / design_doc / mvp_plan を新構成に合わせた。
- llama-server のストリームチャンクで `content: null` が返るケースをハンドリングし、応答テキスト先頭に `None` が出る問題を修正。実際に会話で正常動作することを確認。

## 課題
- 実運用モデルでの応答長さ/口語調の効き具合を未確認。150 字クランプ後の発話・音声品質を要確認。
- キャラクター設定をセッションやユーザー単位で保持する仕組みは未実装（現状はグローバル選択のみ）。
- システムプロンプトも同様に、ユーザー/セッションごとの適用や履歴管理は未実装。
- デバッグUIを react-three-fiber 側に移設し、three-vrm と統合した本番相当の UI/UX を構築する必要がある。

## 次のアクション
- デバッグUIを react-three-fiber 側へ移設し、scene 上で diagnostics/latency/プロバイダ切替を行える UI を実装する。
- 実運用モデルで 10 回程度の対話を行い、150 字超の抑制と会話調が効いているかログ/スクショを取得する。
- 必要に応じてキャラクター選択やシステムプロンプトをセッション/ユーザー設定に紐づける設計を検討する。
