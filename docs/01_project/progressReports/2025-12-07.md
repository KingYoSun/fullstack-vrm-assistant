# 2025年12月07日 進捗レポート

## 概要
- LLM 応答のデフォルトプロンプトを会話調/150字以内に統一し、キャラクター設定とシステムプロンプトを DB で管理・適用できるようにした。
- DGX Spark 向けに gpt-oss-20b (llama.cpp/llama-server) をビルド・起動する構成へ更新し、LLM 応答ストリームの `None` 混入を解消した。

## 成果
- プロンプトビルダーを追加し、WS/Text/Diagnostics すべてでキャラ設定＋RAG 文脈を含むシステムプロンプトを共通化。150 文字超はストリームを打ち切りつつクランプして短文化。
- Postgres に CharacterProfile テーブルを追加し、`/api/v1/characters` CRUD API を実装（名前重複は 409 を返却）。
- Postgres に SystemPrompt テーブルを追加し、`/api/v1/system-prompts` CRUD + active 切替を実装。取得できない場合はデフォルト文へフォールバック。
- フロントに「Conversation Persona」パネルを追加し、キャラの作成/更新/削除・適用を UI で完結。選択したキャラ ID を WS 接続と diagnostics/llm リクエストに付与し、プロンプトへ反映できるようにした。
- フロントにシステムプロンプト管理 UI を追加し、保存/適用/削除をブラウザから操作可能にした。
- LLM を gpt-oss-20b GGUF + llama-server (CUDA 13.0 / SM 12.1) に刷新し、`docker/llm-llama/Dockerfile` と compose の llm サービスを追加。`.env.default` / README / production_runtime / design_doc / mvp_plan を新構成に合わせた。
- llama-server のストリームチャンクで `content: null` が返るケースをハンドリングし、応答テキスト先頭に `None` が出る問題を修正。実際に会話で正常動作することを確認。

## 課題
- 実運用モデルでの応答長さ/口語調の効き具合を未確認。150 字クランプ後の発話・音声品質を要確認。
- キャラクター設定をセッションやユーザー単位で保持する仕組みは未実装（現状はグローバル選択のみ）。
- システムプロンプトも同様に、ユーザー/セッションごとの適用や履歴管理は未実装。

## 次のアクション
- backend を起動し、characters/system-prompts API の CRUD と WS 接続で指定が反映されることを実際に叩いて確認する。
- 実運用モデルで 10 回程度の対話を行い、150 字超の抑制と会話調が効いているかログ/スクショを取得する。
- 必要に応じてキャラクター選択やシステムプロンプトをセッション/ユーザー設定に紐づける設計を検討する。
