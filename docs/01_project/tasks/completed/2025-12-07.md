# 2025年12月07日

- Diagnostics TTS エンドポイントの CORS/OPTIONS (405) でフロントから叩けない問題を修正。
  - FastAPI に `CORSMiddleware` を追加し、プリフライト要求で `Access-Control-Allow-Origin` を返すようにした。
  - 環境変数 `CORS_ALLOWED_ORIGINS` で許可オリジンをカンマ区切り指定できるようにし、`.env.default` にデフォルト値を追加。
- AI秘書の記憶用ディレクトリを `memories/` に新設し、基本機能を記す `basic_func.md` のみ Git 追跡対象とするよう `.gitignore` を更新。
  - `docker-compose.yml` の backend/backend-dev に `./memories` をマウントし、プロジェクトドキュメントではなく記憶専用データを RAG に取り込む構成へ変更。
  - `app.cli.ingest` のデフォルト取り込みディレクトリを `./memories` に変更し、空ディレクトリ時は例外で気付けるようにした。
  - 上記対応後に RAG ingest を実行し、RAG 検索が成功することを確認（次元不一致の解消）。
- フロントエンド VRM カメラ初期位置が足元のみになる問題を修正し、顔正面（肩から上）が映るよう自動調整を追加。
  - VRM ロード完了時に頭部/肩位置からカメラの target と距離を再計算し、初期視点をヘッドショット構図に固定。
  - 近距離での再調整に合わせ OrbitControls の距離レンジを見直し、ボタンで何度でも視点リセットできるようにした。
- バグ修正: 視点リセットボタンを押してもカメラ位置が動かない問題を解消。
  - VRM の向きベクトルを用いてモデル正面からの視点を再計算し、ターゲットだけでなく位置も毎回前面に戻すようにした。
- 調整: 視点リセット時のカメラ距離を段階的に近づけ、頭〜肩上のアップになるよう距離計算の係数を縮小。
- 微調整: ボーン位置の都合で首元が中心になるケースを解消するため、フォーカス点を頭方向に持ち上げ、肩寄り補正を弱めて頭が画面中央に来るようにした。
- WebSocket 会話でマイク入力が mock 表示になる / Assistant 音声が冒頭・末尾で途切れる問題を修正。
  - STT に送信する音声がヘッダ無し PCM のままだったため、WAV に包んで whisper サーバへ渡し mock フォールバックを防止。
  - TTS バイト列の処理を直列化し、フォーマット自動判定＋PCM 時は WAV 化して再生することで音声の欠損を解消（メタデータでサンプルレートを保持）。
- マイク入力の一回録音方式へ変更し、録音開始直後の不要トリガーを防止。
  - 録音開始時に TTS 再生を停止し、MediaRecorder のチャンクは一旦バッファリングして録音終了ボタンでまとめて送信＋`flush` する運用に変更。
  - サーバーへ余分なバイトが送られないため「ご視聴ありがとうございました」などの誤認文字起こしを防ぎ、会話を繰り返しても backlog エラーが出ないことを確認。
- LLM デフォルトプロンプトを会話調・150字以内に統一し、キャラクター設定を Postgres 永続化＋フロント CRUD に対応。
  - 新しいプロンプトビルダーでキャラ設定・コンテキストを統合し、WS/Text/Diagnostics の LLM 応答を 150 文字で打ち切り。CharacterProfile テーブルと CRUD API を追加。
  - フロントにキャラクター管理パネルを追加し、作成/更新/削除と適用を UI から実行。選択したキャラ ID を WS/LLM リクエストに付与してプロンプトへ反映。
- システムプロンプトを PostgreSQL に保存し、CRUD + active 切替を実装。
  - SystemPrompt テーブルと `/api/v1/system-prompts` CRUD API を追加。active プロンプトを取得できない場合はデフォルト文を使用。
  - フロントにシステムプロンプト管理 UI を追加し、保存/適用/削除をブラウザから操作可能にした。
- DGX Spark 向けに gpt-oss-20b を llama-server で起動する llm コンテナを追加し、compose に統合。
  - CUDA 13.0 / SM 12.1 向けに llama.cpp をビルドする `docker/llm-llama/Dockerfile` を追加。
  - `docker-compose.yml` の llm サービスを gpt-oss-20b GGUF 起動（ポート 8080 /health）に差し替え、.env.default のデフォルト値と mock プロファイルも合わせた。
  - README / production_runtime / design_doc / mvp_plan を gpt-oss-20b + llama-server 前提に更新し、LLM 環境変数（ctx-size/threads/n-gpu-layers/flash-attn など）を明示。
- バグ修正: LLM ストリーミング応答の先頭に `None` が混入する問題を解消。
  - llama-server のチャンクで `content: null` が返る場合があり、`None` を文字列化して送信していた。`backend/app/providers/llm.py` の `_extract_content` で null を無視するようにし、応答テキストが正しく始まることを確認。
