llm:
  provider: ${LLM_PROVIDER}
  endpoint: ${LLM_ENDPOINT}
  model: ${LLM_MODEL}
  temperature: ${LLM_TEMPERATURE}
  max_tokens: ${LLM_MAX_TOKENS}
  timeout_sec: ${LLM_TIMEOUT_SEC}
  stream: ${LLM_STREAM}

stt:
  provider: ${STT_PROVIDER}
  endpoint: ${STT_ENDPOINT}
  language: ${STT_LANGUAGE}
  target_sample_rate: ${STT_TARGET_SAMPLE_RATE}
  enable_partial: ${STT_ENABLE_PARTIAL}
  vad: ${STT_VAD}
  timeout_sec: ${STT_TIMEOUT_SEC}

tts:
  provider: ${TTS_PROVIDER}
  endpoint: ${TTS_ENDPOINT}
  default_voice: ${TTS_DEFAULT_VOICE}
  language: ${TTS_LANGUAGE}
  output_format: ${TTS_OUTPUT_FORMAT}
  sample_rate: ${TTS_SAMPLE_RATE}
  stream: ${TTS_STREAM}
  chunk_ms: ${TTS_CHUNK_MS}
  timeout_sec: ${TTS_TIMEOUT_SEC}

rag:
  provider: ${RAG_PROVIDER}
  index_path: ${RAG_INDEX_PATH}
  top_k: ${RAG_TOP_K}
  embedding_provider: ${RAG_EMBEDDING_PROVIDER}

embedding:
  provider: ${EMBEDDING_PROVIDER}
  endpoint: ${EMBEDDING_ENDPOINT}
  model: ${EMBEDDING_MODEL}
  batch_size: ${EMBEDDING_BATCH_SIZE}
  timeout_sec: ${EMBEDDING_TIMEOUT_SEC}
