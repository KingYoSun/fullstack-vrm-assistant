name: fullstack-vrm-assistant

x-common-env: &common-env
  env_file:
    - .env

x-nvidia: &nvidia
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities:
              - gpu

services:
  backend:
    image: vrm-backend:local
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
    container_name: vrm-backend
    restart: unless-stopped
    working_dir: /workspace/backend
    volumes:
      - ./memories:/workspace/memories:ro
      - ./config:/workspace/config:ro
      - ./data:/data
    <<: *common-env
    environment:
      - PROVIDERS_CONFIG_PATH=${PROVIDERS_CONFIG_PATH:-/workspace/config/providers.yaml}
      - PYTHONPATH=/workspace/backend
    ports:
      - "${BACKEND_PORT:-8000}:${BACKEND_PORT:-8000}"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${BACKEND_PORT:-8000}/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - prod

  backend-dev:
    image: vrm-backend:local
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
    container_name: vrm-backend-dev
    working_dir: /workspace/backend
    command: ["sh", "-c", "uvicorn app.main:app --reload --host 0.0.0.0 --port ${BACKEND_PORT:-8000}"]
    volumes:
      - ./backend:/workspace/backend
      - ./memories:/workspace/memories:ro
      - ./config:/workspace/config:ro
      - ./data:/data
    <<: *common-env
    environment:
      - PROVIDERS_CONFIG_PATH=/workspace/config/providers.yaml
      - PYTHONPATH=/workspace/backend
    ports:
      - "${BACKEND_PORT:-8000}:${BACKEND_PORT:-8000}"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${BACKEND_PORT:-8000}/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - dev

  frontend:
    image: vrm-frontend:local
    build:
      context: ./frontend
      dockerfile: ../docker/frontend/Dockerfile
      args:
        VITE_WS_BASE_URL: ${VITE_WS_BASE_URL:-}
        VITE_BACKEND_HOST: ${VITE_BACKEND_HOST:-}
        VITE_BACKEND_PORT: ${VITE_BACKEND_PORT:-8000}
        VITE_WS_PATH: ${VITE_WS_PATH:-/ws/session}
    container_name: vrm-frontend
    restart: unless-stopped
    <<: *common-env
    ports:
      - "${FRONTEND_PORT:-5173}:${FRONTEND_PORT:-5173}"
    depends_on:
      backend:
        condition: service_started
    profiles:
      - prod

  frontend-dev:
    image: vrm-frontend:local
    build:
      context: ./frontend
      dockerfile: ../docker/frontend/Dockerfile
      args:
        VITE_WS_BASE_URL: ${VITE_WS_BASE_URL:-}
        VITE_BACKEND_HOST: ${VITE_BACKEND_HOST:-}
        VITE_BACKEND_PORT: ${VITE_BACKEND_PORT:-8000}
        VITE_WS_PATH: ${VITE_WS_PATH:-/ws/session}
    container_name: vrm-frontend-dev
    working_dir: /workspace/frontend
    command: ["sh", "-c", "pnpm dev --host --port ${FRONTEND_PORT:-5173}"]
    volumes:
      - ./frontend:/workspace/frontend
      - ./frontend/node_modules:/workspace/frontend/node_modules
    <<: *common-env
    ports:
      - "${FRONTEND_PORT:-5173}:${FRONTEND_PORT:-5173}"
    depends_on:
      backend-dev:
        condition: service_started
    profiles:
      - dev

  llm:
    image: ${LLM_IMAGE:-local/gpt-oss-llm:cuda}
    build:
      context: .
      dockerfile: docker/llm-llama/Dockerfile
      target: server
      args:
        UBUNTU_VERSION: "${LLM_UBUNTU_VERSION:-24.04}"
        CUDA_VERSION: "${LLM_CUDA_VERSION:-13.0.2}"
        CUDA_DOCKER_ARCH: "${LLM_CUDA_ARCH:-121}"
        LLAMA_CPP_REPO: "${LLAMA_CPP_REPO:-https://github.com/ggml-org/llama.cpp.git}"
        LLAMA_CPP_REF: "${LLAMA_CPP_REF:-}"
    platform: ${LLM_PLATFORM:-linux/arm64}
    container_name: vrm-llm
    restart: unless-stopped
    <<: *nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ${LLM_MODEL_LOCAL_DIR:-./models/llm}:${LLM_MODEL_DIR:-/models}:ro
    ports:
      - "${LLM_LOCAL_PORT:-18000}:${LLM_PORT:-8080}"
    command:
      - --host
      - 0.0.0.0
      - --port
      - "${LLM_PORT:-8080}"
      - -m
      - "${LLM_MODEL:-/models/gpt-oss-20b.gguf}"
      - --ctx-size
      - "${LLM_CTX_SIZE:-16384}"
      - --threads
      - "${LLM_THREADS:--1}"
      - --n-gpu-layers
      - "${LLM_N_GPU_LAYERS:-99}"
      - --flash-attn
      - "${LLM_FLASH_ATTN:-auto}"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${LLM_PORT:-8080}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - prod
      - dev

  stt:
    image: ${STT_IMAGE:-local/whisper-stt:cuda}
    build:
      context: .
      dockerfile: docker/stt-whisper/Dockerfile
    container_name: vrm-stt
    restart: unless-stopped
    <<: *nvidia
    volumes:
      - ${STT_MODEL_LOCAL_DIR:-./models/stt}:${STT_MODEL_DIR:-/models}
    environment:
      - WHISPER_MODEL=${STT_MODEL:-/models/ggml-base.en.bin}
      - WHISPER_PORT=${STT_PORT:-6006}
      - WHISPER_LANG=${STT_LANGUAGE:-ja}
      - WHISPER_THREADS=${STT_THREADS:-16}
    ports:
      - "${STT_PORT:-6006}:${STT_PORT:-6006}"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${STT_PORT:-6006}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - prod
      - dev

  tts:
    image: ${TTS_IMAGE:-local/openvoice:cuda}
    build:
      context: .
      dockerfile: docker/tts-fish-speech/Dockerfile
    container_name: vrm-tts
    restart: unless-stopped
    <<: *nvidia
    volumes:
      - ${TTS_MODEL_LOCAL_DIR:-./models/tts}:${TTS_MODEL_DIR:-/app/checkpoints}
      - ${TTS_REFERENCE_LOCAL_DIR:-./references/tts}:${TTS_REFERENCE_DIR:-/app/references}
    environment:
      - API_SERVER_NAME=0.0.0.0
      - API_SERVER_PORT=${TTS_PORT:-7007}
      - BACKEND=${TTS_BACKEND:-cuda}
    command: >-
      bash -lc "${TTS_COMMAND:-/app/start_server.sh}"
    ports:
      - "${TTS_PORT:-7007}:${TTS_PORT:-7007}"
    profiles:
      - prod
      - dev

  embedding:
    image: ${EMBEDDING_IMAGE:-local/llama-embedding:cuda}
    build:
      context: .
      dockerfile: docker/embedding-llama/Dockerfile
    container_name: vrm-embedding
    restart: unless-stopped
    <<: *nvidia
    environment:
      - LLAMA_MODEL=${EMBEDDING_MODEL:-/models/embd-model.gguf}
      - LLAMA_PORT=${EMBEDDING_PORT:-9000}
      - LLAMA_PARALLEL=${EMBEDDING_PARALLEL:-4}
      - LLAMA_UBATCH=${EMBEDDING_UBATCH:-1024}
      - LLAMA_NGPU=${EMBEDDING_NGPU:-999}
      - LLAMA_POOLING=${EMBEDDING_POOLING:-mean}
    ports:
      - "${EMBEDDING_PORT:-9000}:${EMBEDDING_PORT:-9000}"
    volumes:
      - ${EMBEDDING_MODEL_LOCAL_DIR:-./models/embedding}:${EMBEDDING_MODEL_DIR:-/models}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${EMBEDDING_PORT:-9000}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - prod
      - dev

  postgres:
    image: postgres:16-alpine
    container_name: vrm-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-vrm}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-vrm_password}
      - POSTGRES_DB=${POSTGRES_DB:-vrm}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-vrm}"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles:
      - prod
      - dev

  llm-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-llm-mock
    profiles:
      - mock
    environment:
      - PORT=${LLM_PORT:-8080}
    ports:
      - "${LLM_LOCAL_PORT:-18000}:${LLM_PORT:-8080}"
    networks:
      default:
        aliases:
          - llm

  stt-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-stt-mock
    profiles:
      - mock
    environment:
      - PORT=${STT_PORT:-6006}
    ports:
      - "${STT_PORT:-6006}:${STT_PORT:-6006}"
    networks:
      default:
        aliases:
          - stt

  tts-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-tts-mock
    profiles:
      - mock
    environment:
      - PORT=${TTS_PORT:-7007}
    ports:
      - "${TTS_PORT:-7007}:${TTS_PORT:-7007}"
    networks:
      default:
        aliases:
          - tts

  embedding-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-embedding-mock
    profiles:
      - mock
    environment:
      - PORT=${EMBEDDING_PORT:-9000}
    ports:
      - "${EMBEDDING_PORT:-9000}:${EMBEDDING_PORT:-9000}"
    networks:
      default:
        aliases:
          - embedding

volumes:
  postgres_data:
  frontend_node_modules:
