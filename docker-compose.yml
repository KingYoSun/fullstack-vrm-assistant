name: fullstack-vrm-assistant

x-common-env: &common-env
  env_file:
    - .env

x-nvidia: &nvidia
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities:
              - gpu

services:
  backend:
    image: python:3.12-slim
    container_name: vrm-backend
    restart: unless-stopped
    working_dir: /workspace/backend
    command: >-
      sh -c "
        cd /workspace/backend &&
        apt-get update &&
        DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends ffmpeg build-essential curl &&
        rm -rf /var/lib/apt/lists/* &&
        python -m pip install --no-cache-dir --upgrade pip setuptools wheel &&
        if [ -f requirements.dev.txt ]; then
          PIP_NO_BUILD_ISOLATION=1 pip install --no-cache-dir -r requirements.dev.txt;
        else
          echo 'requirements.dev.txt が見つかりません。依存を追加後に再実行してください。';
        fi &&
        if [ -f app/main.py ]; then
          uvicorn app.main:app --host 0.0.0.0 --port ${BACKEND_PORT:-8000};
        else
          echo 'app/main.py が見つかりません。バックエンド初期化後に再起動してください。' &&
          tail -f /dev/null;
        fi
      "
    volumes:
      - ./backend:/workspace/backend
      - ./config:/workspace/config:ro
      - ./data:/data
    <<: *common-env
    environment:
      - PROVIDERS_CONFIG_PATH=${PROVIDERS_CONFIG_PATH:-/workspace/config/providers.yaml}
      - PYTHONPATH=/workspace/backend
    ports:
      - "${BACKEND_PORT:-8000}:${BACKEND_PORT:-8000}"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${BACKEND_PORT:-8000}/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  frontend:
    image: node:20-bullseye
    container_name: vrm-frontend
    restart: unless-stopped
    working_dir: /workspace/frontend
    command: >-
      sh -c "
        cd /workspace/frontend &&
        corepack enable &&
        if [ -f package.json ]; then
          pnpm install --frozen-lockfile=false &&
          pnpm build &&
          pnpm preview -- --host --port ${FRONTEND_PORT:-5173};
        else
          echo 'package.json が見つかりません。Vite+React 初期化後に再起動してください。' &&
          tail -f /dev/null;
        fi
      "
    volumes:
      - ./frontend:/workspace/frontend
      - frontend_node_modules:/workspace/frontend/node_modules
    <<: *common-env
    ports:
      - "${FRONTEND_PORT:-5173}:${FRONTEND_PORT:-5173}"
    depends_on:
      backend:
        condition: service_started

  llm:
    image: ${LLM_IMAGE:-nvcr.io/nim/qwen/qwen3-32b-dgx-spark:latest}
    platform: ${LLM_PLATFORM:-linux/aarch64}
    container_name: vrm-llm
    restart: unless-stopped
    <<: *nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NGC_API_KEY=${NGC_API_KEY}
    volumes:
      - ${LLM_MODEL_LOCAL_DIR:-./models/llm}:${LLM_MODEL_DIR:-/opt/nim/workspace}
    ports:
      - "${LLM_LOCAL_PORT:-18000}:${LLM_PORT:-8000}"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:18000/v1/health/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  stt:
    image: ${STT_IMAGE:-local/whisper-stt:cuda}
    build:
      context: .
      dockerfile: docker/stt-whisper/Dockerfile
    container_name: vrm-stt
    restart: unless-stopped
    <<: *nvidia
    volumes:
      - ${STT_MODEL_LOCAL_DIR:-./models/stt}:${STT_MODEL_DIR:-/models}
    environment:
      - WHISPER_MODEL=${STT_MODEL:-/models/ggml-base.en.bin}
      - WHISPER_PORT=${STT_PORT:-6006}
      - WHISPER_LANG=${STT_LANGUAGE:-ja}
      - WHISPER_THREADS=${STT_THREADS:-16}
    ports:
      - "${STT_PORT:-6006}:${STT_PORT:-6006}"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${STT_PORT:-6006}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  tts:
    image: ${TTS_IMAGE:-local/openvoice:cuda}
    build:
      context: .
      dockerfile: docker/tts-fish-speech/Dockerfile
    container_name: vrm-tts
    restart: unless-stopped
    <<: *nvidia
    volumes:
      - ${TTS_MODEL_LOCAL_DIR:-./models/tts}:${TTS_MODEL_DIR:-/app/checkpoints}
      - ${TTS_REFERENCE_LOCAL_DIR:-./references/tts}:${TTS_REFERENCE_DIR:-/app/references}
    environment:
      - API_SERVER_NAME=0.0.0.0
      - API_SERVER_PORT=${TTS_PORT:-7007}
      - BACKEND=${TTS_BACKEND:-cuda}
    command: >-
      bash -lc "${TTS_COMMAND:-/app/start_server.sh}"
    ports:
      - "${TTS_PORT:-7007}:${TTS_PORT:-7007}"

  embedding:
    image: ${EMBEDDING_IMAGE:-local/llama-embedding:cuda}
    build:
      context: .
      dockerfile: docker/embedding-llama/Dockerfile
    container_name: vrm-embedding
    restart: unless-stopped
    <<: *nvidia
    environment:
      - LLAMA_MODEL=${EMBEDDING_MODEL:-/models/embd-model.gguf}
      - LLAMA_PORT=${EMBEDDING_PORT:-9000}
      - LLAMA_PARALLEL=${EMBEDDING_PARALLEL:-4}
      - LLAMA_UBATCH=${EMBEDDING_UBATCH:-1024}
      - LLAMA_NGPU=${EMBEDDING_NGPU:-999}
      - LLAMA_POOLING=${EMBEDDING_POOLING:-mean}
    ports:
      - "${EMBEDDING_PORT:-9000}:${EMBEDDING_PORT:-9000}"
    volumes:
      - ${EMBEDDING_MODEL_LOCAL_DIR:-./models/embedding}:${EMBEDDING_MODEL_DIR:-/models}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${EMBEDDING_PORT:-9000}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  postgres:
    image: postgres:16-alpine
    container_name: vrm-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-vrm}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-vrm_password}
      - POSTGRES_DB=${POSTGRES_DB:-vrm}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-vrm}"]
      interval: 10s
      timeout: 5s
      retries: 5

  llm-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-llm-mock
    profiles:
      - mock
    environment:
      - PORT=${LLM_PORT:-18000}
    ports:
      - "${LLM_PORT:-18000}:${LLM_PORT:-18000}"
    networks:
      default:
        aliases:
          - llm

  stt-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-stt-mock
    profiles:
      - mock
    environment:
      - PORT=${STT_PORT:-6006}
    ports:
      - "${STT_PORT:-6006}:${STT_PORT:-6006}"
    networks:
      default:
        aliases:
          - stt

  tts-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-tts-mock
    profiles:
      - mock
    environment:
      - PORT=${TTS_PORT:-7007}
    ports:
      - "${TTS_PORT:-7007}:${TTS_PORT:-7007}"
    networks:
      default:
        aliases:
          - tts

  embedding-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-embedding-mock
    profiles:
      - mock
    environment:
      - PORT=${EMBEDDING_PORT:-9000}
    ports:
      - "${EMBEDDING_PORT:-9000}:${EMBEDDING_PORT:-9000}"
    networks:
      default:
        aliases:
          - embedding

volumes:
  frontend_node_modules:
  postgres_data:
