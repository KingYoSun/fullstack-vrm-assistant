name: fullstack-vrm-assistant

x-common-env: &common-env
  env_file:
    - .env

x-nvidia: &nvidia
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities:
              - gpu

services:
  backend:
    image: vrm-backend:local
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
    container_name: vrm-backend
    restart: unless-stopped
    working_dir: /workspace/backend
    volumes:
      - ./config:/workspace/config:ro
      - ./data:/data
    <<: *common-env
    environment:
      - PROVIDERS_CONFIG_PATH=${PROVIDERS_CONFIG_PATH:-/workspace/config/providers.yaml}
      - PYTHONPATH=/workspace/backend
    ports:
      - "${BACKEND_PORT:-8000}:${BACKEND_PORT:-8000}"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${BACKEND_PORT:-8000}/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  frontend:
    image: vrm-frontend:local
    build:
      context: ./frontend
      dockerfile: ../docker/frontend/Dockerfile
    container_name: vrm-frontend
    restart: unless-stopped
    <<: *common-env
    ports:
      - "${FRONTEND_PORT:-5173}:${FRONTEND_PORT:-5173}"
    depends_on:
      backend:
        condition: service_started

  llm:
    image: ${LLM_IMAGE:-nvcr.io/nim/qwen/qwen3-32b-dgx-spark:latest}
    platform: ${LLM_PLATFORM:-linux/aarch64}
    container_name: vrm-llm
    restart: unless-stopped
    <<: *nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NGC_API_KEY=${NGC_API_KEY}
    volumes:
      - ${LLM_MODEL_LOCAL_DIR:-./models/llm}:${LLM_MODEL_DIR:-/opt/nim/workspace}
    ports:
      - "${LLM_LOCAL_PORT:-18000}:${LLM_PORT:-8000}"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:18000/v1/health/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  stt:
    image: ${STT_IMAGE:-local/whisper-stt:cuda}
    build:
      context: .
      dockerfile: docker/stt-whisper/Dockerfile
    container_name: vrm-stt
    restart: unless-stopped
    <<: *nvidia
    volumes:
      - ${STT_MODEL_LOCAL_DIR:-./models/stt}:${STT_MODEL_DIR:-/models}
    environment:
      - WHISPER_MODEL=${STT_MODEL:-/models/ggml-base.en.bin}
      - WHISPER_PORT=${STT_PORT:-6006}
      - WHISPER_LANG=${STT_LANGUAGE:-ja}
      - WHISPER_THREADS=${STT_THREADS:-16}
    ports:
      - "${STT_PORT:-6006}:${STT_PORT:-6006}"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${STT_PORT:-6006}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  tts:
    image: ${TTS_IMAGE:-local/openvoice:cuda}
    build:
      context: .
      dockerfile: docker/tts-fish-speech/Dockerfile
    container_name: vrm-tts
    restart: unless-stopped
    <<: *nvidia
    volumes:
      - ${TTS_MODEL_LOCAL_DIR:-./models/tts}:${TTS_MODEL_DIR:-/app/checkpoints}
      - ${TTS_REFERENCE_LOCAL_DIR:-./references/tts}:${TTS_REFERENCE_DIR:-/app/references}
    environment:
      - API_SERVER_NAME=0.0.0.0
      - API_SERVER_PORT=${TTS_PORT:-7007}
      - BACKEND=${TTS_BACKEND:-cuda}
    command: >-
      bash -lc "${TTS_COMMAND:-/app/start_server.sh}"
    ports:
      - "${TTS_PORT:-7007}:${TTS_PORT:-7007}"

  embedding:
    image: ${EMBEDDING_IMAGE:-local/llama-embedding:cuda}
    build:
      context: .
      dockerfile: docker/embedding-llama/Dockerfile
    container_name: vrm-embedding
    restart: unless-stopped
    <<: *nvidia
    environment:
      - LLAMA_MODEL=${EMBEDDING_MODEL:-/models/embd-model.gguf}
      - LLAMA_PORT=${EMBEDDING_PORT:-9000}
      - LLAMA_PARALLEL=${EMBEDDING_PARALLEL:-4}
      - LLAMA_UBATCH=${EMBEDDING_UBATCH:-1024}
      - LLAMA_NGPU=${EMBEDDING_NGPU:-999}
      - LLAMA_POOLING=${EMBEDDING_POOLING:-mean}
    ports:
      - "${EMBEDDING_PORT:-9000}:${EMBEDDING_PORT:-9000}"
    volumes:
      - ${EMBEDDING_MODEL_LOCAL_DIR:-./models/embedding}:${EMBEDDING_MODEL_DIR:-/models}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${EMBEDDING_PORT:-9000}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  postgres:
    image: postgres:16-alpine
    container_name: vrm-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-vrm}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-vrm_password}
      - POSTGRES_DB=${POSTGRES_DB:-vrm}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-vrm}"]
      interval: 10s
      timeout: 5s
      retries: 5

  llm-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-llm-mock
    profiles:
      - mock
    environment:
      - PORT=${LLM_PORT:-18000}
    ports:
      - "${LLM_PORT:-18000}:${LLM_PORT:-18000}"
    networks:
      default:
        aliases:
          - llm

  stt-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-stt-mock
    profiles:
      - mock
    environment:
      - PORT=${STT_PORT:-6006}
    ports:
      - "${STT_PORT:-6006}:${STT_PORT:-6006}"
    networks:
      default:
        aliases:
          - stt

  tts-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-tts-mock
    profiles:
      - mock
    environment:
      - PORT=${TTS_PORT:-7007}
    ports:
      - "${TTS_PORT:-7007}:${TTS_PORT:-7007}"
    networks:
      default:
        aliases:
          - tts

  embedding-mock:
    image: docker.io/ealen/echo-server:latest
    container_name: vrm-embedding-mock
    profiles:
      - mock
    environment:
      - PORT=${EMBEDDING_PORT:-9000}
    ports:
      - "${EMBEDDING_PORT:-9000}:${EMBEDDING_PORT:-9000}"
    networks:
      default:
        aliases:
          - embedding

volumes:
  postgres_data:
